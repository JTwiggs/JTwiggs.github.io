---
layout: post
title: Week 12 - Putting the Pieces Together
date: 2025-03-26 14:28:26
description: Data collection made easy with Crawl4AI
tags: seniorProject crawl4ai
categories: sample-posts
featured: true
---

### This Semester is Wrapping Up! - Week 12

Things are really getting down to the wire; I finally got a reliable way of extracting data from websites that is ingestible and instantaneously prepped for AI use. This has been done by way of <a href ='https://github.com/unclecode/crawl4ai'>Crawl4AI</a> as was mentioned in last week's post. At this point, I donâ€™t think that there will be an additional table or dataset being loaded for the model to access, since I feel that this is working well enough for a demo. RAG models are made to check markdown files, not necessarily call predictive models. We still have a couple weeks, so it might be possible, but that is no longer something I am aiming for. The goal for the presentation is to also have a user interface (UI) so people can see some of the application in real time. The UI will be built with <a href='https://streamlit.io/'>Streamlit</a>. 

If you are curious to hear about the process of completing this project, and problems that got me stuck along the way, stick around for the rest of the post! This one is going to be a bit longer than the others, but I think it will be worth it. I will also be including some of the code that I used to get the data from the web and into the model.

#### Step 1: Getting the Data
This proved to be the most difficult part of the project, which was not what I expected when I started this whole project. I figured that web scraping was already proficient enough at what I was needing, and it wouldn't be much trouble to find something that worked. I have had a bit of experience with webscraping prior to this project, so I thought it would be a breeze. I was wrong. I spent a lot of time trying to get the data from the web, and it was not until I found Crawl4AI that I was able to get something that worked. The code for this is below, and it is pretty simple to use. You just need to install the package and follow the quick start guide. I did make some significant changes to the code, and I will screenshot the base crawler compared to the one that ended up working for me. The logic behind it is pretty simple, and I will explain it below. 

Here is the base crawler that you can just copy and paste from the Crawl4AI GitHub page. It is a bit long, but it is pretty simple to understand. The only thing that you need to change is the URL that you want to scrape. I used the UFC website, and I will show you how I did it.

```{python}
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com")
        print(result.markdown[:300])  # Print first 300 chars

if __name__ == "__main__":
    asyncio.run(main())
```

Very impressive for about 10 lines of code, right? This is as simple as it gets if you need website data. The only thing that you need to change is the URL that you want to scrape. I used the UFC website, and my code is somewhat reflective of that. So without further ado, here is a snippet of the code I used. If you want to see the full code, you can check out my GitHub page. I will be posting the full code there once I finish this project, and it will eventually be much more organized than it is now. I have several things I want to implement still, so while it will be available, it will not be the final product.


This is what the code that I used to get the data from the web ended up looking like, or at least a section of it.
```{python} 
 try:
        session_id = "session1"  # Reuse the same session across all URLs
        while True:
            if level == 0:
                print('starting main page url collection')
                for url in urls:
                    result = await crawler.arun(
                        url=url,
                        config=crawl_config,
                        session_id=session_id
                    )
                    if result.success:
                        print(f"Successfully crawled: {url}")
                        # E.g. check markdown length
                        print(f"Markdown length: {len(result.markdown.raw_markdown)}")
                    else:
                        print(f"Failed: {url} - Error: {result.error_message}")
                    with open('ufc_base_scrape_results.md', 'w', encoding="utf-8") as f:
                        print(result.markdown, file=f)
                    with open('ufc_base_scrape_results.md', 'r', encoding="utf-8") as f:
                        lines = f.readlines()
                        lines = lines[:-3] #remove footer from the file
                urls = [line.split(" | ")[0].strip("<>") for line in lines]
                # Use list comprehension to filter only athlete URLs (this is where any custom filtering can be done)
                pattern = re.compile(r'https://www.ufc.com/athlete[^\s|>]*')
                matches = [url for url in urls if pattern.match(url)]
                # Save the filtered URLs
                with open("athlete_info_from_main.txt", "a") as f:
                    for match in matches:
                        print(match, file=f)
                # Remove the last line from the file
                with open("fighter_urls.txt", "r") as f:
                    lines = f.readlines()
                    lines = lines[:-1] #remove extra line from the file
                print('figher urls collected')
                level += 1
            else:
```

I'm confident that more sophisticated methods could have been employed, but a for loop with a couple of changes seemed to work for me. Something I learned as I was building this out is that sitemaps only exist at the highest level of a website, which was a huge upset in the moment. I wanted to start at a more specific page of a website and work my way down, but that was not possible. I had to start at the main page and work my way down from there. This is what started the for loop, as I realized the base page is essentially an advertisement for links to other popular or upcoming fights and events. Once that initial loop was complete, I filtered the resulting content to only include the fighter URLs, as that was the only thing I wanted the RAG model to be accessing. It took a few hours to get this working, as I haven't really needed for loops for many of my recent projects, and getting the regex, syntax, and logic to flow was time consuming. I'm definitely more confident in my scraping abilities now, and I do think that since I was able to get this simple workflow completed, I can convert it to a more general solution so it can build RAG models for any website and collect the right information. This is where I will be spending a bit of time to get the project to a point that I am confident in displaying it on github as a project employers can see. Streamlit is not difficult to use in the slightest, so depending on how well my descent into that rabbit hole goes, I may have a working demo in time for the presentation. I'm editing and finishing this post with only about 8 hours before the conference, so while it is unlikely, I am going to try my best to have a fun demo by the end of the day.