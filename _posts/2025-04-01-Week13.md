---
layout: post
title: Week 13 - Conference Week
date: 2025-04-01 21:01:00
description:  Final week of preparations; poster, presentation, conclusion 
tags: seniorProject poster
categories: sample-posts
featured: true
---

With a model working adequately as of Monday, I am super excited to say I have finally decided on what would be the most beneficial area to focus on with my presentation at this conference: How web crawling can bolster the usefulness of RAG models. I need to come up with a better title, one that is a bit more reflective of Crawl4AI’s impact on this project, and somehow tie in the UFC if possible. This went from creating a fun little tool to a much more research based idea. I feel that getting an RAG working with (technically) a single, relatively short, markdown file is already a win. I need to test it some more tomorrow, but the results I was getting looked very promising. I’m still mildly undecided on whether I cater towards the idea of an AI tool, or research specific presentation. Essentially, this semester of researching has lead to three key points that I want to get across in a professional way this week:

While I built this workflow to collect information from the ufc homepage, some additional customization could make this collect information about almost any website, given the time to complete a full query. If the files are saved and managed effectively, a personal database of useful websites to use as text embeddings for the model can become extremely powerful. For example, crawling medicare.gov and connecting an RAG model to both a chatbot and a text-to-speech (tts) model would provide genuine value in answering plan specific questions immediately, and add an additional filter on top of those already in place, ensuring that calls that reach agents are using time more effectively, and providing more value to customers.

Webscraping is a huge can of worms. Crawl4AI is a package that I happily stumbled across after trying to use other tools and resources for almost a month. I needed clean (enough) markdown files to give to a model so context could be retained and recalled from embeddings, and this managed to simplify the whole experience. The documentation made everything easy to implement, and it is free, which is honestly mind blowing. 
Customized agentic AI tools have so much potential to save time. Instead of needing all questions routed to someone who knows niche and specific documented information, LLM-based agents have blasted the ceiling wide open in terms of tools and repetitive workflows that can be automated now. I plan to implement some of these ideas during my upcoming internship with an insurance company based out of Southern Utah, and I anticipate each tool saving the company nearly 40 hours of customer support each week, allowing for additional earning potential.

I want to take a bit of time to dive into the RAG architecture and how it works. Here's a picture that outlines it simply, and I will explain each of the steps below.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/RAG_pipeline.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

We need a user query to start off with. This can be just about anything, but the nature of RAG models works better with questions about a specific topic, and assumes that the user has an idea of what type of data the model is being trained on. An example with my model is that users know that it has access to the UFC website, and can ask questions about fighters, fights, or events. The model will then take the query and search through the embeddings that have been created from the markdown files that were scraped from the website. But before we even get to the embeddings, we need to get the data to create embeddings from.

I used UFC.com as the target website to pull data from, and I didn't know that there were so many different tools for the task. RAG models work best when creating embeddings from very well formatted markdown files or simple text files. With adjustments to the chunk size being embedded, these seem to be the only two formats that work well in my experience, since the model 'reads' the data given to it and builds context accordingly. If you give it a bunch of random text or page metadata, it doesn't really know what kind of context to give, making it less effective. Once the data is collected, it is chunked into small pieces and then embedded to a database. I am grossly oversimplifying this process, but as someone who has always loved the application more than the theory, its going to stay that simple for now. Suffice it to say that once the data is prepared in this way, the model can then use it to generate a response to the user query. This is done by using a combination of natural language processing (NLP) and machine learning (ML) techniques to generate a response that is relevant to the user query.

I want to go into embeddings a bit more, since embeddings are the unsung heroes of this whole process. Without text and/or vectorized embeddings, the model can't understand the context behind each of the chunks of text that it has saved and been trained on. When I was finalizing the test model, adjusting the chunk size allowed for a much better contextual understanding of the data. The model will take the user query and search through the embeddings that have been created from the markdown files that were scraped from the website. 

Now that a query has been made, and relevant chunks have been located in the database, the model will then use these chunks to generate a response. This is done by using a combination of natural language processing (NLP) and machine learning (ML) techniques to generate a response that is relevant to the user query. It is up to the model designer to choose an AI to orchestrate the response, but I used OpenAI's GPT-2.0-mini, as it is one of the more snappy response generators. Gemini models, mistral, and Claude are other options, but I have not had the chance to test them out yet. I will be sure to update this post once I have a chance to test them out, as I am very curious about how they compare to the OpenAI models.

And that is basically all the major steps of the whole pipeline! Once I design a dashboard to make the whole process general, I think it will be a useful tool for learning relevant data quickly, as Cole Medin showcased with the model he designed to scan github repositories and give specific details about the code. There is a lot of potential here for genuinely valuable chat bots and AI tools, and I am excited to see where this goes in the future.


<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" controls=true %}
    </div>
</div> --->
<!-- <div class="caption">
    Here are a couple of the videos I found really helpful with learning more about not only web crawling, but also text embeddings and RAG models. I am still working on the poster, but I will be sure to include these in the references section.
</div>

It does also support embedding videos from different sources. Here are some examples:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="https://www.youtube.com/watch?v=GjR5UsVGE60" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="https://www.youtube.com/watch?v=tcqEUSNCn8I&t=731s&pp=ygUMcGl4ZWdhbWkgcmFn" class="img-fluid rounded z-depth-1" %}
    </div> 
     <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="https://player.vimeo.com/video/524933864?h=1ac4fd9fb4&title=0&byline=0&portrait=0" class="img-fluid rounded z-depth-1" %}
    </div> -->

