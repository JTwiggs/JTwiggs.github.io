<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jtwiggs.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jtwiggs.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-03T19:17:13+00:00</updated><id>https://jtwiggs.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Week 13 - Conference Week</title><link href="https://jtwiggs.github.io/blog/2025/Week13/" rel="alternate" type="text/html" title="Week 13 - Conference Week"/><published>2025-04-01T21:01:00+00:00</published><updated>2025-04-01T21:01:00+00:00</updated><id>https://jtwiggs.github.io/blog/2025/Week13</id><content type="html" xml:base="https://jtwiggs.github.io/blog/2025/Week13/"><![CDATA[<p>With a model working adequately as of Monday, I am super excited to say I have finally decided on what would be the most beneficial area to focus on with my presentation at this conference: How web crawling can bolster the usefulness of RAG models. I need to come up with a better title, one that is a bit more reflective of Crawl4AI’s impact on this project, and somehow tie in the UFC if possible. This went from creating a fun little tool to a much more research based idea. I feel that getting an RAG working with (technically) a single, relatively short, markdown file is already a win. I need to test it some more tomorrow, but the results I was getting looked very promising. I’m still mildly undecided on whether I cater towards the idea of an AI tool, or research specific presentation. Essentially, this semester of researching has lead to three key points that I want to get across in a professional way this week:</p> <p>While I built this workflow to collect information from the ufc homepage, some additional customization could make this collect information about almost any website, given the time to complete a full query. If the files are saved and managed effectively, a personal database of useful websites to use as text embeddings for the model can become extremely powerful. For example, crawling medicare.gov and connecting an RAG model to both a chatbot and a text-to-speech (tts) model would provide genuine value in answering plan specific questions immediately, and add an additional filter on top of those already in place, ensuring that calls that reach agents are using time more effectively, and providing more value to customers.</p> <p>Webscraping is a huge can of worms. Crawl4AI is a package that I happily stumbled across after trying to use other tools and resources for almost a month. I needed clean (enough) markdown files to give to a model so context could be retained and recalled from embeddings, and this managed to simplify the whole experience. The documentation made everything easy to implement, and it is free, which is honestly mind blowing. Customized agentic AI tools have so much potential to save time. Instead of needing all questions routed to someone who knows niche and specific documented information, LLM-based agents have blasted the ceiling wide open in terms of tools and repetitive workflows that can be automated now. I plan to implement some of these ideas during my upcoming internship with an insurance company based out of Southern Utah, and I anticipate each tool saving the company nearly 40 hours of customer support each week, allowing for additional earning potential.</p> <p>I want to take a bit of time to dive into the RAG architecture and how it works. Here’s a picture that outlines it simply, and I will explain each of the steps below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/RAG_pipeline-480.webp 480w,/assets/img/RAG_pipeline-800.webp 800w,/assets/img/RAG_pipeline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/RAG_pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We need a user query to start off with. This can be just about anything, but the nature of RAG models works better with questions about a specific topic, and assumes that the user has an idea of what type of data the model is being trained on. An example with my model is that users know that it has access to the UFC website, and can ask questions about fighters, fights, or events. The model will then take the query and search through the embeddings that have been created from the markdown files that were scraped from the website. But before we even get to the embeddings, we need to get the data to create embeddings from.</p> <p>I used UFC.com as the target website to pull data from, and I didn’t know that there were so many different tools for the task. RAG models work best when creating embeddings from very well formatted markdown files or simple text files. With adjustments to the chunk size being embedded, these seem to be the only two formats that work well in my experience, since the model ‘reads’ the data given to it and builds context accordingly. If you give it a bunch of random text or page metadata, it doesn’t really know what kind of context to give, making it less effective. Once the data is collected, it is chunked into small pieces and then embedded to a database. I am grossly oversimplifying this process, but as someone who has always loved the application more than the theory, its going to stay that simple for now. Suffice it to say that once the data is prepared in this way, the model can then use it to generate a response to the user query. This is done by using a combination of natural language processing (NLP) and machine learning (ML) techniques to generate a response that is relevant to the user query.</p> <p>I want to go into embeddings a bit more, since embeddings are the unsung heroes of this whole process. Without text and/or vectorized embeddings, the model can’t understand the context behind each of the chunks of text that it has saved and been trained on. When I was finalizing the test model, adjusting the chunk size allowed for a much better contextual understanding of the data. The model will take the user query and search through the embeddings that have been created from the markdown files that were scraped from the website.</p> <p>Now that a query has been made, and relevant chunks have been located in the database, the model will then use these chunks to generate a response. This is done by using a combination of natural language processing (NLP) and machine learning (ML) techniques to generate a response that is relevant to the user query. It is up to the model designer to choose an AI to orchestrate the response, but I used OpenAI’s GPT-2.0-mini, as it is one of the more snappy response generators. Gemini models, mistral, and Claude are other options, but I have not had the chance to test them out yet. I will be sure to update this post once I have a chance to test them out, as I am very curious about how they compare to the OpenAI models.</p> <p>And that is basically all the major steps of the whole pipeline! Once I design a dashboard to make the whole process general, I think it will be a useful tool for learning relevant data quickly, as Cole Medin showcased with the model he designed to scan github repositories and give specific details about the code. There is a lot of potential here for genuinely valuable chat bots and AI tools, and I am excited to see where this goes in the future.</p> ]]></content><author><name></name></author><category term="sample-posts"/><category term="seniorProject"/><category term="poster"/><summary type="html"><![CDATA[Final week of preparations; poster, presentation, conclusion]]></summary></entry><entry><title type="html">Week 10 - More RAG Research</title><link href="https://jtwiggs.github.io/blog/2025/Week10/" rel="alternate" type="text/html" title="Week 10 - More RAG Research"/><published>2025-03-26T14:28:26+00:00</published><updated>2025-03-26T14:28:26+00:00</updated><id>https://jtwiggs.github.io/blog/2025/Week10</id><content type="html" xml:base="https://jtwiggs.github.io/blog/2025/Week10/"><![CDATA[<h3 id="researching-rags">Researching RAG’s</h3> <p>This week wasn’t great so far as progress. The more I try and understand how RAG models work, the less easy it is for me to understand where I need to be headed. With only a month left before the R&amp;CW event, I don’t think I will be hitting the goals I had set initially when setting out for a senior project. Instead of air quality analysis, the project has leaned much more heavily into AI development. Instead of worrying about a predictive model to compare answers to, I am planning to either scrape data from a secondary source for the model to compare to, or just leave it at 1. Other classes are eating time away from this project, but I’m still planning to have a deliverable on time!</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="seniorProject"/><category term="n8n"/><summary type="html"><![CDATA[There is way more to this than I thought]]></summary></entry><entry><title type="html">Week 12 - Putting the Pieces Together</title><link href="https://jtwiggs.github.io/blog/2025/week12/" rel="alternate" type="text/html" title="Week 12 - Putting the Pieces Together"/><published>2025-03-26T14:28:26+00:00</published><updated>2025-03-26T14:28:26+00:00</updated><id>https://jtwiggs.github.io/blog/2025/week12</id><content type="html" xml:base="https://jtwiggs.github.io/blog/2025/week12/"><![CDATA[<h3 id="this-semester-is-wrapping-up---week-12">This Semester is Wrapping Up! - Week 12</h3> <p>Things are really getting down to the wire; I finally got a reliable way of extracting data from websites that is ingestible and instantaneously prepped for AI use. This has been done by way of <a href="https://github.com/unclecode/crawl4ai">Crawl4AI</a> as was mentioned in last week’s post. At this point, I don’t think that there will be an additional table or dataset being loaded for the model to access, since I feel that this is working well enough for a demo. RAG models are made to check markdown files, not necessarily call predictive models. We still have a couple weeks, so it might be possible, but that is no longer something I am aiming for. The goal for the presentation is to also have a user interface (UI) so people can see some of the application in real time. The UI will be built with <a href="https://streamlit.io/">Streamlit</a>.</p> <p>If you are curious to hear about the process of completing this project, and problems that got me stuck along the way, stick around for the rest of the post! This one is going to be a bit longer than the others, but I think it will be worth it. I will also be including some of the code that I used to get the data from the web and into the model.</p> <h4 id="step-1-getting-the-data">Step 1: Getting the Data</h4> <p>This proved to be the most difficult part of the project, which was not what I expected when I started this whole project. I figured that web scraping was already proficient enough at what I was needing, and it wouldn’t be much trouble to find something that worked. I have had a bit of experience with webscraping prior to this project, so I thought it would be a breeze. I was wrong. I spent a lot of time trying to get the data from the web, and it was not until I found Crawl4AI that I was able to get something that worked. The code for this is below, and it is pretty simple to use. You just need to install the package and follow the quick start guide. I did make some significant changes to the code, and I will screenshot the base crawler compared to the one that ended up working for me. The logic behind it is pretty simple, and I will explain it below.</p> <p>Here is the base crawler that you can just copy and paste from the Crawl4AI GitHub page. It is a bit long, but it is pretty simple to understand. The only thing that you need to change is the URL that you want to scrape. I used the UFC website, and I will show you how I did it.</p> <pre><code class="language-{python}">import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com")
        print(result.markdown[:300])  # Print first 300 chars

if __name__ == "__main__":
    asyncio.run(main())
</code></pre> <p>Very impressive for about 10 lines of code, right? This is as simple as it gets if you need website data. The only thing that you need to change is the URL that you want to scrape. I used the UFC website, and my code is somewhat reflective of that. So without further ado, here is a snippet of the code I used. If you want to see the full code, you can check out my GitHub page. I will be posting the full code there once I finish this project, and it will eventually be much more organized than it is now. I have several things I want to implement still, so while it will be available, it will not be the final product.</p> <p>This is what the code that I used to get the data from the web ended up looking like, or at least a section of it.</p> <pre><code class="language-{python}"> try:
        session_id = "session1"  # Reuse the same session across all URLs
        while True:
            if level == 0:
                print('starting main page url collection')
                for url in urls:
                    result = await crawler.arun(
                        url=url,
                        config=crawl_config,
                        session_id=session_id
                    )
                    if result.success:
                        print(f"Successfully crawled: {url}")
                        # E.g. check markdown length
                        print(f"Markdown length: {len(result.markdown.raw_markdown)}")
                    else:
                        print(f"Failed: {url} - Error: {result.error_message}")
                    with open('ufc_base_scrape_results.md', 'w', encoding="utf-8") as f:
                        print(result.markdown, file=f)
                    with open('ufc_base_scrape_results.md', 'r', encoding="utf-8") as f:
                        lines = f.readlines()
                        lines = lines[:-3] #remove footer from the file
                urls = [line.split(" | ")[0].strip("&lt;&gt;") for line in lines]
                # Use list comprehension to filter only athlete URLs (this is where any custom filtering can be done)
                pattern = re.compile(r'https://www.ufc.com/athlete[^\s|&gt;]*')
                matches = [url for url in urls if pattern.match(url)]
                # Save the filtered URLs
                with open("athlete_info_from_main.txt", "a") as f:
                    for match in matches:
                        print(match, file=f)
                # Remove the last line from the file
                with open("fighter_urls.txt", "r") as f:
                    lines = f.readlines()
                    lines = lines[:-1] #remove extra line from the file
                print('figher urls collected')
                level += 1
            else:
</code></pre> <p>I’m confident that more sophisticated methods could have been employed, but a for loop with a couple of changes seemed to work for me. Something I learned as I was building this out is that sitemaps only exist at the highest level of a website, which was a huge upset in the moment. I wanted to start at a more specific page of a website and work my way down, but that was not possible. I had to start at the main page and work my way down from there. This is what started the for loop, as I realized the base page is essentially an advertisement for links to other popular or upcoming fights and events. Once that initial loop was complete, I filtered the resulting content to only include the fighter URLs, as that was the only thing I wanted the RAG model to be accessing. It took a few hours to get this working, as I haven’t really needed for loops for many of my recent projects, and getting the regex, syntax, and logic to flow was time consuming. I’m definitely more confident in my scraping abilities now, and I do think that since I was able to get this simple workflow completed, I can convert it to a more general solution so it can build RAG models for any website and collect the right information. This is where I will be spending a bit of time to get the project to a point that I am confident in displaying it on github as a project employers can see. Streamlit is not difficult to use in the slightest, so depending on how well my descent into that rabbit hole goes, I may have a working demo in time for the presentation. I’m editing and finishing this post with only about 8 hours before the conference, so while it is unlikely, I am going to try my best to have a fun demo by the end of the day.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="seniorProject"/><category term="crawl4ai"/><summary type="html"><![CDATA[Data collection made easy with Crawl4AI]]></summary></entry><entry><title type="html">Week 11 - Crawl4AI</title><link href="https://jtwiggs.github.io/blog/2025/Week11/" rel="alternate" type="text/html" title="Week 11 - Crawl4AI"/><published>2025-03-19T21:01:00+00:00</published><updated>2025-03-19T21:01:00+00:00</updated><id>https://jtwiggs.github.io/blog/2025/Week11</id><content type="html" xml:base="https://jtwiggs.github.io/blog/2025/Week11/"><![CDATA[<p>This has been one of the best weeks of the semester so far as progress goes. I’ve found a much more specific tool to use for this project in the python library <a href="https://github.com/unclecode/crawl4ai">Crawl4AI</a>. Some of the key features include:</p> <ul> <li>asynchronous crawling</li> <li>Markdown formatting</li> <li>Content targeting</li> <li>much, much more…</li> </ul> <p>This package has extensive documentation and I would highly recommend checking it out. Moving on to the resource that lead to me finding Crawl4AI in the first place, <a href="https://www.youtube.com/@ColeMedin">Cole Medin’s channel</a> on YouTube. He not only has really insightful videos on agentic AI, but his project ideas are fascinating to follow along with. I’ve made reference to a couple of them in my research poster, and a link to the video that introduced me to his content is linked below. I feel like there is a great balance between technical explanations and beginner explanations. Additionally, <a href="https://www.youtube.com/@pixegami">pixelgami</a> is another channel that has great tutorials for creating RAG models in a simply explained way. I’ve also linked the video that was extremely helpful in this project below. I haven’t quite gotten the code working as of this post, so I will continue working on that and hopefully have a working model by next week. &lt;!– &lt;div class="row mt-3"&gt; &lt;div class="col-sm mt-3 mt-md-0"&gt;</p> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div></div> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div></div> <p>&lt;/div&gt;</p> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div> <p>It does also support embedding videos from different sources. Here are some examples: –&gt;</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/watch?v=U6LbW2IFUQw&amp;list=PLyrg3m7Ei-MqNM-au_lfjzTsuVVTTx0ke&amp;index=4&amp;pp=iAQB" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/watch?v=tcqEUSNCn8I&amp;t=731s" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <div class="caption"> Here are the videos I found to be fairly insightful and helpful in building out the model and tools I needed. </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="videos"/><category term="webScraping"/><summary type="html"><![CDATA[Crawl4AI and other awesome resources]]></summary></entry><entry><title type="html">Week 8 - Career Fair &amp;amp; Resume</title><link href="https://jtwiggs.github.io/blog/2025/Week8/" rel="alternate" type="text/html" title="Week 8 - Career Fair &amp;amp; Resume"/><published>2025-03-05T14:28:26+00:00</published><updated>2025-03-05T14:28:26+00:00</updated><id>https://jtwiggs.github.io/blog/2025/Week8</id><content type="html" xml:base="https://jtwiggs.github.io/blog/2025/Week8/"><![CDATA[<h3 id="career-fair">Career Fair</h3> <p>There was a huge career fair this week, and while I am still feeling pretty under the weather, I felt that it would have been a huge waste not to go and talk to some potential employers about internship opportunities. Writing about this from a week later, I unfortunately missed some basic (but small) errors on my resume, but I had some printed, and got to hand out about 5 at the event. On top of that, I was better able to see what languages, skills, and directions that some of these companies were looking for. For all the applications and potential, I only got two interviews out of it. The biggest drawback of my resume is lacking work experience, and I need work experience to land a job… This is such a fun game. Anyways I want to take a minute and mention some of the biggest things that helped improve my resume.</p> <p>The absolute best thing was letting other people look at my resume. They are able to spot small mistakes and errors, but they also can give great advice for better ways of wording things, more presentable formats, and ideas for future improvements. I showed my resume to many friends and faculty, and they gave me good feedback every time. Rarely did I leave from one of these informal meetings with, “wow, your resume is so good it can’t get any better.” The harder people dig at your resume, the better you can make it.</p> <p>Another method of improving your resume, especially when you are a college student, is to limit information provided to specifically the job you are applying for. In other words, don’t list every single job, experience, and event you have every attended on your resume. Try and really think about what the recruiter or manager would want in an applicant, and try to highlight only those things. Even if the resume ends up being slightly less than a page, it helps the recruiter know exactly what you are good at rather than every single thing you kind of know how to do.</p> <p>Finally, don’t use colors needlessly. You can change font colors to match the company’s logo if it makes sense, but absolutely do not use more than two colors, and if you are going to do that, make one the header color where your name is, and the other for the whole rest of the document. There is an element of design and style when building a resume, but not anything extravagant.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="seniorProject"/><category term="portfolio"/><category term="resume"/><summary type="html"><![CDATA[Project work will be resumed next week]]></summary></entry><entry><title type="html">Week 9 - New Portfolio Site</title><link href="https://jtwiggs.github.io/blog/2025/Week9/" rel="alternate" type="text/html" title="Week 9 - New Portfolio Site"/><published>2025-03-05T14:28:26+00:00</published><updated>2025-03-05T14:28:26+00:00</updated><id>https://jtwiggs.github.io/blog/2025/Week9</id><content type="html" xml:base="https://jtwiggs.github.io/blog/2025/Week9/"><![CDATA[<h3 id="researching-rags">Researching RAG’s</h3> <p>This week I set about learning now to work with jekyll themes, and have chosen a new template to host my portfolio with. The Al-folio template is very easy to understand, and looks great to me. I spent most of my time this week getting a hand on what can and cannot be edited within each of the pages. Well… Everything can be customized, but I am not versed in web development so it felt like I was breaking code far more often than I was customizing it. By next week, I should have my about me page and resume page updated to reflect me instead of good old Albert Einstein.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="seniorProject"/><category term="portfolio"/><category term="gh-pages"/><summary type="html"><![CDATA[Al-folio is the way to go]]></summary></entry></feed>